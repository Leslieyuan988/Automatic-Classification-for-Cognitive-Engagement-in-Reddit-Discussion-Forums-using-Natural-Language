{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4fb246",
   "metadata": {},
   "source": [
    "# Evaluate the result of topic modeling for unigram, bigram and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8f08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "# from spellchecker import SpellChecker\n",
    "import spacy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a8479",
   "metadata": {},
   "source": [
    "# N-Gram\n",
    "- unigram\n",
    "- bigram\n",
    "- trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbff16a",
   "metadata": {},
   "source": [
    "### First Attempt - Topic Modeling (Unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "175a0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"unigrams\"\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "data_unigram = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4397a069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['okay',\n",
       "  'tone',\n",
       "  'volume',\n",
       "  'accent',\n",
       "  'vocabulary',\n",
       "  'emotions',\n",
       "  'physical',\n",
       "  'movement'],\n",
       " ['gtyou', 'made', 'snort', 'snorting', 'crack', 'class'],\n",
       " ['ah', 'peasant', 'spirit', 'alive', 'well', 'see'],\n",
       " ['interesting', 'bet', 'pros', 'would', 'love', 'training'],\n",
       " ['think', 'unacceptable', 'every', 'context']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unigram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aba1cedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAVA: [['tone', 'volume', 'vocabulary', 'emotion', 'physical', 'movement'], ['snort', 'crack', 'class'], ['peasant', 'spirit', 'alive', 'well'], ['interesting', 'bet', 'pro', 'training'], ['unacceptable', 'context'], ['ever', 'cookie'], ['long', 'feeling', 'wrong'], ['even', 'botw', 'lmao', 'recommendation', 'terrible'], ['never', 'reason', 'use'], ['work', 'holy', 'one'], ['first', 'half'], ['odst', 'proper', 'multiplayer', 'right', 'firefight'], ['star', 'rating'], ['quick', 'game'], ['appreciate'], ['suspect', 'theory', 'already', 'phase'], ['quality', 'game'], ['good', 'world'], ['thing', 'ever'], ['sun', 'radiation', 'outside']]\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "nlp.max_length = 1400000\n",
    "\n",
    "# Do lemmatization keeping noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_unigram, allowed_postags=['NOUN', 'ADJ', 'VB', 'ADV'])\n",
    "print(\"NAVA:\", data_lemmatized[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7c4777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus_new = [id2word.doc2bow(text) for text in texts] # BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6796bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus_new = [id2word.doc2bow(text) for text in texts] # BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a21b668c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"music\" + 0.014*\"well\" + 0.013*\"reddit\" + 0.012*\"post\" + 0.012*\"place\" + 0.012*\"community\" + 0.011*\"new\" + 0.010*\"feedback\" + 0.010*\"way\" + 0.009*\"question\"'),\n",
       " (1,\n",
       "  '0.025*\"people\" + 0.011*\"never\" + 0.008*\"even\" + 0.008*\"time\" + 0.007*\"well\" + 0.007*\"year\" + 0.007*\"really\" + 0.007*\"life\" + 0.007*\"kid\" + 0.007*\"way\"'),\n",
       " (2,\n",
       "  '0.016*\"game\" + 0.016*\"thing\" + 0.013*\"good\" + 0.013*\"time\" + 0.010*\"even\" + 0.008*\"day\" + 0.008*\"still\" + 0.008*\"man\" + 0.008*\"also\" + 0.008*\"friend\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_3 = models.LdaModel(corpus=corpus_new, num_topics=3, id2word=id2word, passes=10)\n",
    "lda_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19ef08d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"music\" + 0.018*\"reddit\" + 0.013*\"user\" + 0.012*\"comment\" + 0.011*\"man\" + 0.010*\"rmusic\" + 0.010*\"friend\" + 0.009*\"submit\" + 0.008*\"place\" + 0.008*\"help\"'),\n",
       " (1,\n",
       "  '0.027*\"people\" + 0.016*\"much\" + 0.013*\"feedback\" + 0.013*\"good\" + 0.012*\"well\" + 0.010*\"place\" + 0.010*\"positive\" + 0.009*\"able\" + 0.009*\"thing\" + 0.009*\"even\"'),\n",
       " (2,\n",
       "  '0.025*\"way\" + 0.020*\"day\" + 0.020*\"game\" + 0.015*\"community\" + 0.013*\"good\" + 0.012*\"even\" + 0.011*\"thing\" + 0.011*\"share\" + 0.009*\"life\" + 0.009*\"character\"'),\n",
       " (3,\n",
       "  '0.040*\"time\" + 0.011*\"post\" + 0.011*\"question\" + 0.011*\"action\" + 0.010*\"new\" + 0.009*\"year\" + 0.008*\"rule\" + 0.008*\"subreddit\" + 0.008*\"well\" + 0.007*\"even\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_4 = models.LdaModel(corpus=corpus_new, num_topics=4, id2word=id2word, passes=10)\n",
    "lda_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8e793e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"system\" + 0.014*\"post\" + 0.011*\"place\" + 0.011*\"good\" + 0.010*\"rule\" + 0.010*\"subreddit\" + 0.009*\"world\" + 0.009*\"well\" + 0.008*\"content\" + 0.008*\"instead\"'),\n",
       " (1,\n",
       "  '0.025*\"music\" + 0.021*\"reddit\" + 0.018*\"new\" + 0.015*\"question\" + 0.014*\"character\" + 0.013*\"share\" + 0.013*\"rmusic\" + 0.011*\"comment\" + 0.011*\"submit\" + 0.010*\"alignment\"'),\n",
       " (2,\n",
       "  '0.022*\"feedback\" + 0.016*\"place\" + 0.010*\"person\" + 0.009*\"concern\" + 0.009*\"advice\" + 0.008*\"ampx\" + 0.008*\"gt\" + 0.008*\"party\" + 0.007*\"member\" + 0.007*\"already\"'),\n",
       " (3,\n",
       "  '0.034*\"people\" + 0.015*\"game\" + 0.012*\"much\" + 0.012*\"player\" + 0.012*\"even\" + 0.011*\"child\" + 0.011*\"man\" + 0.011*\"thing\" + 0.010*\"level\" + 0.009*\"also\"'),\n",
       " (4,\n",
       "  '0.024*\"time\" + 0.017*\"year\" + 0.017*\"well\" + 0.016*\"community\" + 0.016*\"thing\" + 0.014*\"day\" + 0.013*\"bad\" + 0.013*\"really\" + 0.013*\"good\" + 0.012*\"user\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_5 = models.LdaModel(corpus=corpus_new, num_topics=5, id2word=id2word, passes=10)\n",
    "lda_5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4343810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score for 3 topics:  0.65676271077902\n",
      "\n",
      "Coherence Score for 4 topics:  0.5428848292453103\n",
      "\n",
      "Coherence Score for 5 topics:  0.4730728000659042\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score for the base model\n",
    "coherence_model_lda = CoherenceModel(model=lda_3, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 3 topics: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_4, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 4 topics: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_5, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 5 topics: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d11b8",
   "metadata": {},
   "source": [
    "### Second Attempt - Topic Modeling (Bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41bf79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"bigrams\"\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "data_bigram = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa27d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['okay',\n",
       "  'tone',\n",
       "  'volume',\n",
       "  'accent',\n",
       "  'vocabulary',\n",
       "  'emotions',\n",
       "  'physical',\n",
       "  'movement'],\n",
       " ['gtyou', 'made', 'snort', 'snorting', 'crack', 'class'],\n",
       " ['ah', 'peasant', 'spirit', 'alive', 'well', 'see'],\n",
       " ['interesting', 'bet', 'pros', 'would', 'love', 'training'],\n",
       " ['think', 'unacceptable', 'every', 'context']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bigram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da66a289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAVA: [['tone', 'volume', 'vocabulary', 'emotion', 'physical', 'movement'], ['snort', 'crack', 'class'], ['peasant', 'spirit', 'alive', 'well'], ['interesting', 'bet', 'pro', 'training'], ['unacceptable', 'context'], ['ever', 'cookie'], ['long', 'feeling', 'wrong'], ['even', 'botw', 'lmao', 'recommendation', 'terrible'], ['never', 'reason', 'use'], ['work', 'holy', 'one'], ['first', 'half'], ['odst', 'proper', 'multiplayer', 'right', 'firefight'], ['star', 'rating'], ['quick', 'game'], ['appreciate'], ['suspect', 'theory', 'already', 'phase'], ['quality', 'game'], ['good', 'world'], ['thing', 'ever'], ['sun', 'radiation', 'outside']]\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized = lemmatization(data_bigram, allowed_postags=['NOUN', 'ADJ', 'VB', 'ADV'])\n",
    "print(\"NAVA:\", data_lemmatized[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "986fad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus_new = [id2word.doc2bow(text) for text in texts] # BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cc5625a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"game\" + 0.014*\"people\" + 0.011*\"year\" + 0.011*\"even\" + 0.009*\"also\" + 0.008*\"system\" + 0.007*\"good\" + 0.007*\"player\" + 0.006*\"money\" + 0.006*\"alignment\"'),\n",
       " (1,\n",
       "  '0.017*\"well\" + 0.015*\"place\" + 0.013*\"reddit\" + 0.013*\"way\" + 0.011*\"question\" + 0.010*\"share\" + 0.010*\"comment\" + 0.010*\"rmusic\" + 0.009*\"day\" + 0.009*\"new\"'),\n",
       " (2,\n",
       "  '0.025*\"time\" + 0.013*\"good\" + 0.012*\"thing\" + 0.010*\"really\" + 0.008*\"day\" + 0.008*\"back\" + 0.008*\"still\" + 0.008*\"much\" + 0.008*\"well\" + 0.008*\"man\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_3 = models.LdaModel(corpus=corpus_new, num_topics=3, id2word=id2word, passes=10)\n",
    "lda_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "555b7b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"people\" + 0.011*\"good\" + 0.011*\"actually\" + 0.010*\"man\" + 0.010*\"also\" + 0.010*\"time\" + 0.009*\"way\" + 0.008*\"child\" + 0.008*\"alignment\" + 0.008*\"much\"'),\n",
       " (1,\n",
       "  '0.018*\"place\" + 0.014*\"game\" + 0.014*\"year\" + 0.012*\"bad\" + 0.012*\"people\" + 0.012*\"thing\" + 0.010*\"even\" + 0.010*\"never\" + 0.010*\"day\" + 0.009*\"player\"'),\n",
       " (2,\n",
       "  '0.020*\"still\" + 0.018*\"way\" + 0.018*\"well\" + 0.014*\"new\" + 0.012*\"share\" + 0.012*\"rmusic\" + 0.011*\"time\" + 0.010*\"music\" + 0.008*\"day\" + 0.007*\"feeling\"'),\n",
       " (3,\n",
       "  '0.014*\"question\" + 0.014*\"post\" + 0.014*\"time\" + 0.013*\"thing\" + 0.013*\"comment\" + 0.012*\"good\" + 0.010*\"issue\" + 0.009*\"reddit\" + 0.009*\"help\" + 0.009*\"action\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_4 = models.LdaModel(corpus=corpus_new, num_topics=4, id2word=id2word, passes=10)\n",
    "lda_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13746f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"also\" + 0.018*\"time\" + 0.015*\"good\" + 0.013*\"character\" + 0.011*\"well\" + 0.011*\"much\" + 0.010*\"thing\" + 0.009*\"back\" + 0.009*\"problem\" + 0.008*\"really\"'),\n",
       " (1,\n",
       "  '0.020*\"game\" + 0.019*\"thing\" + 0.015*\"even\" + 0.014*\"people\" + 0.012*\"share\" + 0.011*\"way\" + 0.011*\"good\" + 0.011*\"music\" + 0.010*\"really\" + 0.008*\"bad\"'),\n",
       " (2,\n",
       "  '0.023*\"year\" + 0.019*\"people\" + 0.017*\"well\" + 0.015*\"community\" + 0.014*\"player\" + 0.013*\"time\" + 0.013*\"system\" + 0.013*\"alignment\" + 0.011*\"level\" + 0.010*\"bit\"'),\n",
       " (3,\n",
       "  '0.027*\"day\" + 0.013*\"rmusic\" + 0.012*\"way\" + 0.012*\"new\" + 0.012*\"able\" + 0.011*\"time\" + 0.010*\"child\" + 0.009*\"kid\" + 0.009*\"help\" + 0.009*\"subreddit\"'),\n",
       " (4,\n",
       "  '0.027*\"place\" + 0.022*\"reddit\" + 0.020*\"well\" + 0.018*\"question\" + 0.018*\"post\" + 0.016*\"man\" + 0.012*\"comment\" + 0.012*\"user\" + 0.010*\"rule\" + 0.010*\"also\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_5 = models.LdaModel(corpus=corpus_new, num_topics=5, id2word=id2word, passes=10)\n",
    "lda_5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a11abdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score for 3 topics:  0.6008988395515505\n",
      "\n",
      "Coherence Score for 4 topics:  0.5848013109391921\n",
      "\n",
      "Coherence Score for 5 topics:  0.5291158043887213\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score for the base model\n",
    "coherence_model_lda = CoherenceModel(model=lda_3, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 3 topics: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_4, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 4 topics: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_5, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 5 topics: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06f3f9",
   "metadata": {},
   "source": [
    "### Third Attempt - Topic Modeling (Trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9f662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"trigrams\"\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "data_trigram = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc2eb811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['okay',\n",
       "  'tone',\n",
       "  'volume',\n",
       "  'accent',\n",
       "  'vocabulary',\n",
       "  'emotions',\n",
       "  'physical',\n",
       "  'movement'],\n",
       " ['gtyou', 'made', 'snort', 'snorting', 'crack', 'class'],\n",
       " ['ah', 'peasant', 'spirit', 'alive', 'well', 'see'],\n",
       " ['interesting', 'bet', 'pros', 'would', 'love', 'training'],\n",
       " ['think', 'unacceptable', 'every', 'context']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trigram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c314982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAVA: [['tone', 'volume', 'vocabulary', 'emotion', 'physical', 'movement'], ['snort', 'crack', 'class'], ['peasant', 'spirit', 'alive', 'well'], ['interesting', 'bet', 'pro', 'training'], ['unacceptable', 'context'], ['ever', 'cookie'], ['long', 'feeling', 'wrong'], ['even', 'botw', 'lmao', 'recommendation', 'terrible'], ['never', 'reason', 'use'], ['work', 'holy', 'one'], ['first', 'half'], ['odst', 'proper', 'multiplayer', 'right', 'firefight'], ['star', 'rating'], ['quick', 'game'], ['appreciate'], ['suspect', 'theory', 'already', 'phase'], ['quality', 'game'], ['good', 'world'], ['thing', 'ever'], ['sun', 'radiation', 'outside']]\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized = lemmatization(data_trigram, allowed_postags=['NOUN', 'ADJ', 'VB', 'ADV'])\n",
    "print(\"NAVA:\", data_lemmatized[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dee00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus_new = [id2word.doc2bow(text) for text in texts] # BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9272abe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"time\" + 0.016*\"game\" + 0.012*\"even\" + 0.009*\"also\" + 0.008*\"well\" + 0.007*\"still\" + 0.007*\"year\" + 0.007*\"player\" + 0.007*\"good\" + 0.007*\"much\"'),\n",
       " (1,\n",
       "  '0.019*\"people\" + 0.014*\"good\" + 0.013*\"thing\" + 0.009*\"character\" + 0.009*\"bad\" + 0.008*\"really\" + 0.008*\"life\" + 0.007*\"friend\" + 0.007*\"never\" + 0.007*\"kid\"'),\n",
       " (2,\n",
       "  '0.016*\"place\" + 0.016*\"well\" + 0.012*\"new\" + 0.012*\"time\" + 0.011*\"reddit\" + 0.011*\"question\" + 0.011*\"post\" + 0.010*\"day\" + 0.010*\"comment\" + 0.009*\"action\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_3 = models.LdaModel(corpus=corpus_new, num_topics=3, id2word=id2word, passes=10)\n",
    "lda_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d240034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"good\" + 0.016*\"thing\" + 0.015*\"new\" + 0.013*\"character\" + 0.012*\"question\" + 0.012*\"post\" + 0.012*\"part\" + 0.011*\"time\" + 0.010*\"action\" + 0.009*\"community\"'),\n",
       " (1,\n",
       "  '0.013*\"man\" + 0.013*\"year\" + 0.012*\"time\" + 0.009*\"maybe\" + 0.008*\"still\" + 0.008*\"big\" + 0.008*\"guy\" + 0.008*\"woman\" + 0.007*\"month\" + 0.006*\"mind\"'),\n",
       " (2,\n",
       "  '0.019*\"people\" + 0.018*\"way\" + 0.014*\"game\" + 0.013*\"even\" + 0.013*\"also\" + 0.012*\"never\" + 0.011*\"time\" + 0.010*\"thing\" + 0.009*\"friend\" + 0.009*\"bad\"'),\n",
       " (3,\n",
       "  '0.019*\"place\" + 0.016*\"well\" + 0.016*\"day\" + 0.014*\"reddit\" + 0.011*\"able\" + 0.011*\"music\" + 0.010*\"rmusic\" + 0.008*\"first\" + 0.007*\"submit\" + 0.007*\"even\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_4 = models.LdaModel(corpus=corpus_new, num_topics=4, id2word=id2word, passes=10)\n",
    "lda_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "741e686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.030*\"time\" + 0.023*\"year\" + 0.014*\"reason\" + 0.010*\"bit\" + 0.010*\"person\" + 0.009*\"positive\" + 0.008*\"high\" + 0.007*\"rule\" + 0.007*\"relationship\" + 0.007*\"album\"'),\n",
       " (1,\n",
       "  '0.027*\"game\" + 0.018*\"day\" + 0.015*\"time\" + 0.013*\"people\" + 0.012*\"system\" + 0.010*\"level\" + 0.009*\"different\" + 0.009*\"player\" + 0.008*\"even\" + 0.008*\"user\"'),\n",
       " (2,\n",
       "  '0.017*\"reddit\" + 0.016*\"question\" + 0.016*\"place\" + 0.016*\"post\" + 0.013*\"new\" + 0.013*\"able\" + 0.012*\"kid\" + 0.012*\"action\" + 0.012*\"comment\" + 0.012*\"child\"'),\n",
       " (3,\n",
       "  '0.028*\"people\" + 0.015*\"well\" + 0.014*\"even\" + 0.014*\"way\" + 0.011*\"many\" + 0.011*\"music\" + 0.011*\"community\" + 0.009*\"alignment\" + 0.009*\"also\" + 0.009*\"money\"'),\n",
       " (4,\n",
       "  '0.029*\"good\" + 0.018*\"thing\" + 0.017*\"right\" + 0.014*\"character\" + 0.012*\"man\" + 0.012*\"really\" + 0.011*\"much\" + 0.009*\"world\" + 0.009*\"never\" + 0.009*\"also\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_5 = models.LdaModel(corpus=corpus_new, num_topics=5, id2word=id2word, passes=10)\n",
    "lda_5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "760e76b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score for 3 topics:  0.633579925051611\n",
      "\n",
      "Coherence Score for 4 topics:  0.6018338278500548\n",
      "\n",
      "Coherence Score for 5 topics:  0.5534405169808639\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score for the base model\n",
    "coherence_model_lda = CoherenceModel(model=lda_3, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 3 topics: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_4, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 4 topics: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_5, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 5 topics: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba659c7b",
   "metadata": {},
   "source": [
    "### Insights:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
