{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f8af10",
   "metadata": {},
   "source": [
    "# Automatic Classification for Cognitive Engagement in Reddit Discussion Forums using Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80fdf27",
   "metadata": {},
   "source": [
    "The project aims to change the traditional framework of education system and make improvement to teaching methods for better learning. We can distinguish four levels of cognitive engagement:\n",
    "        \n",
    "                            Passive, Active, Constructive, Interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d82e5d",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbcbfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>kind</th>\n",
       "      <th>category</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_subscriber</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>score</th>\n",
       "      <th>created</th>\n",
       "      <th>num_crossposts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s2npif</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1642040007</td>\n",
       "      <td>rogmexico</td>\n",
       "      <td>t2_16qf7m</td>\n",
       "      <td>t3_s2npif</td>\n",
       "      <td>t5_2sptq</td>\n",
       "      <td>659490</td>\n",
       "      <td>datascience</td>\n",
       "      <td>How much of your workload is \"assigned\" to you...</td>\n",
       "      <td>**TLDR my questions:**\\n\\n* **How much of your...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1642040007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2nlf6</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1642039703</td>\n",
       "      <td>bikeskata</td>\n",
       "      <td>t2_c7y1n44w</td>\n",
       "      <td>t3_s2nlf6</td>\n",
       "      <td>t5_2sptq</td>\n",
       "      <td>659490</td>\n",
       "      <td>datascience</td>\n",
       "      <td>An approachable introduction to the Bayesian o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>https://solomonkurz.netlify.app/post/2021-12-2...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1642039703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s2m7g3</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1642035730</td>\n",
       "      <td>i_am_baldilocks</td>\n",
       "      <td>t2_aewcc</td>\n",
       "      <td>t3_s2m7g3</td>\n",
       "      <td>t5_2sptq</td>\n",
       "      <td>659490</td>\n",
       "      <td>datascience</td>\n",
       "      <td>Finding Part-Time DS Work</td>\n",
       "      <td>Hey guys,\\n\\nDoes anyone know how to find part...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1642035730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s2l23y</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1642032331</td>\n",
       "      <td>DoctorQuinlan</td>\n",
       "      <td>t2_y5s32</td>\n",
       "      <td>t3_s2l23y</td>\n",
       "      <td>t5_2sptq</td>\n",
       "      <td>659490</td>\n",
       "      <td>datascience</td>\n",
       "      <td>Data Science vs Data Engineer jobs/salary/expe...</td>\n",
       "      <td>So I currently work as a DB programmer, which ...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1642032331</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s2i9br</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1642024923</td>\n",
       "      <td>toomaime</td>\n",
       "      <td>t2_o4jqp</td>\n",
       "      <td>t3_s2i9br</td>\n",
       "      <td>t5_2sptq</td>\n",
       "      <td>659490</td>\n",
       "      <td>datascience</td>\n",
       "      <td>Sports Analytics company Hudl is looking for a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>https://sportekjobs.com/data-scientist-applied...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1642024923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id kind  category  created_utc           author author_fullname  \\\n",
       "0  s2npif   t3       NaN   1642040007        rogmexico       t2_16qf7m   \n",
       "1  s2nlf6   t3       NaN   1642039703        bikeskata     t2_c7y1n44w   \n",
       "2  s2m7g3   t3       NaN   1642035730  i_am_baldilocks        t2_aewcc   \n",
       "3  s2l23y   t3       NaN   1642032331    DoctorQuinlan        t2_y5s32   \n",
       "4  s2i9br   t3       NaN   1642024923         toomaime        t2_o4jqp   \n",
       "\n",
       "        name subreddit_id  subreddit_subscriber    subreddit  \\\n",
       "0  t3_s2npif     t5_2sptq                659490  datascience   \n",
       "1  t3_s2nlf6     t5_2sptq                659490  datascience   \n",
       "2  t3_s2m7g3     t5_2sptq                659490  datascience   \n",
       "3  t3_s2l23y     t5_2sptq                659490  datascience   \n",
       "4  t3_s2i9br     t5_2sptq                659490  datascience   \n",
       "\n",
       "                                               title  \\\n",
       "0  How much of your workload is \"assigned\" to you...   \n",
       "1  An approachable introduction to the Bayesian o...   \n",
       "2                          Finding Part-Time DS Work   \n",
       "3  Data Science vs Data Engineer jobs/salary/expe...   \n",
       "4  Sports Analytics company Hudl is looking for a...   \n",
       "\n",
       "                                            selftext  upvote_ratio  \\\n",
       "0  **TLDR my questions:**\\n\\n* **How much of your...          1.00   \n",
       "1                                                NaN          1.00   \n",
       "2  Hey guys,\\n\\nDoes anyone know how to find part...          0.83   \n",
       "3  So I currently work as a DB programmer, which ...          0.83   \n",
       "4                                                NaN          0.67   \n",
       "\n",
       "                                                 url  num_comments  ups  \\\n",
       "0  https://www.reddit.com/r/datascience/comments/...             1    2   \n",
       "1  https://solomonkurz.netlify.app/post/2021-12-2...             0    2   \n",
       "2  https://www.reddit.com/r/datascience/comments/...             8   11   \n",
       "3  https://www.reddit.com/r/datascience/comments/...            11    8   \n",
       "4  https://sportekjobs.com/data-scientist-applied...             0    1   \n",
       "\n",
       "   downs  total_awards_received  score     created  num_crossposts  \n",
       "0      0                      0      2  1642040007               0  \n",
       "1      0                      0      2  1642039703               0  \n",
       "2      0                      0     11  1642035730               0  \n",
       "3      0                      0      8  1642032331               0  \n",
       "4      0                      0      1  1642024923               0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "comments_df = pd.read_csv('Submissions.csv')\n",
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4119be8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>send_replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('h91cqc4',)</td>\n",
       "      <td>('Street-Spot5011',)</td>\n",
       "      <td>('AskReddit',)</td>\n",
       "      <td>(1629105358,)</td>\n",
       "      <td>('Personally, I’d talk to him abt it in a CIVI...</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('/r/AskReddit/comments/p4u4i2/if_you_saw_some...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('h91cufe',)</td>\n",
       "      <td>('Puechamp',)</td>\n",
       "      <td>('AskReddit',)</td>\n",
       "      <td>(1629105416,)</td>\n",
       "      <td>('Slap his face real hard. Never burn a book i...</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('/r/AskReddit/comments/p4u4i2/if_you_saw_some...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('h91cvff',)</td>\n",
       "      <td>('4AcidRayne',)</td>\n",
       "      <td>('AskReddit',)</td>\n",
       "      <td>(1629105429,)</td>\n",
       "      <td>('Add that person to my mental \"let\\'s keep an...</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('/r/AskReddit/comments/p4u4i2/if_you_saw_some...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('h91cwch',)</td>\n",
       "      <td>('hate_most_of_you',)</td>\n",
       "      <td>('AskReddit',)</td>\n",
       "      <td>(1629105442,)</td>\n",
       "      <td>(\"do notn'tn't\",)</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('t1_h91caod',)</td>\n",
       "      <td>('/r/AskReddit/comments/p4u4i2/if_you_saw_some...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('h91cxbj',)</td>\n",
       "      <td>('JacksonBoyd12',)</td>\n",
       "      <td>('AskReddit',)</td>\n",
       "      <td>(1629105455,)</td>\n",
       "      <td>('Cheer them on and give them another Bible to...</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('t3_p4u4i2',)</td>\n",
       "      <td>('/r/AskReddit/comments/p4u4i2/if_you_saw_some...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     comment_id                 author       subreddit   retrieved_on  \\\n",
       "0  ('h91cqc4',)   ('Street-Spot5011',)  ('AskReddit',)  (1629105358,)   \n",
       "1  ('h91cufe',)          ('Puechamp',)  ('AskReddit',)  (1629105416,)   \n",
       "2  ('h91cvff',)        ('4AcidRayne',)  ('AskReddit',)  (1629105429,)   \n",
       "3  ('h91cwch',)  ('hate_most_of_you',)  ('AskReddit',)  (1629105442,)   \n",
       "4  ('h91cxbj',)     ('JacksonBoyd12',)  ('AskReddit',)  (1629105455,)   \n",
       "\n",
       "                                        comment_text         link_id  \\\n",
       "0  ('Personally, I’d talk to him abt it in a CIVI...  ('t3_p4u4i2',)   \n",
       "1  ('Slap his face real hard. Never burn a book i...  ('t3_p4u4i2',)   \n",
       "2  ('Add that person to my mental \"let\\'s keep an...  ('t3_p4u4i2',)   \n",
       "3                                  (\"do notn'tn't\",)  ('t3_p4u4i2',)   \n",
       "4  ('Cheer them on and give them another Bible to...  ('t3_p4u4i2',)   \n",
       "\n",
       "         parent_id                                          permalink  \\\n",
       "0   ('t3_p4u4i2',)  ('/r/AskReddit/comments/p4u4i2/if_you_saw_some...   \n",
       "1   ('t3_p4u4i2',)  ('/r/AskReddit/comments/p4u4i2/if_you_saw_some...   \n",
       "2   ('t3_p4u4i2',)  ('/r/AskReddit/comments/p4u4i2/if_you_saw_some...   \n",
       "3  ('t1_h91caod',)  ('/r/AskReddit/comments/p4u4i2/if_you_saw_some...   \n",
       "4   ('t3_p4u4i2',)  ('/r/AskReddit/comments/p4u4i2/if_you_saw_some...   \n",
       "\n",
       "   send_replies  \n",
       "0          True  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4          True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv('CommentStreams.csv')\n",
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeeae1e",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb797a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id_x  Name_x subject_id  id_y Name_y\n",
      "0     1    Alex       sub1   NaN    NaN\n",
      "1     2     Amy       sub2   1.0  Billy\n",
      "2     2     Amy       sub2   2.0  Brian\n",
      "3     2     Amy       sub2   3.0   Bran\n",
      "4     3   Allen       sub4   NaN    NaN\n",
      "5     4   Alice       sub6   4.0  Bryce\n",
      "6     5  Ayoung       sub5   5.0  Betty\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "left = pd.DataFrame({\n",
    "   'id':[1,2,3,4,5],\n",
    "   'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'],\n",
    "   'subject_id':['sub1','sub2','sub4','sub6','sub5']})\n",
    "right = pd.DataFrame({\n",
    "   'id':[1,2,3,4,5],\n",
    "   'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'],\n",
    "   'subject_id':['sub2','sub2','sub2','sub6','sub5']})\n",
    "print(pd.merge(left, right, on='subject_id', how='left'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae65ee4",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78059e6",
   "metadata": {},
   "source": [
    "## Discussion Context Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15088426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab12e9b",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f1ada",
   "metadata": {},
   "source": [
    "The main objective of doc2vec is to convert sentence or paragraph to vector (numeric) form. In Natural Language Processing Doc2Vec is used to find related sentences for a given sentence (instead of word in Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91efcf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'data', 'science'],\n",
       " ['i', 'love', 'coding', 'in', 'python'],\n",
       " ['i', 'love', 'building', 'nlp', 'tool'],\n",
       " ['this', 'is', 'a', 'good', 'phone'],\n",
       " ['this', 'is', 'a', 'good', 'tv'],\n",
       " ['this', 'is', 'a', 'good', 'laptop']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## Exapmple document (list of sentences)\n",
    "doc = [\"I love data science\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building NLP tool\",\n",
    "        \"This is a good phone\",\n",
    "        \"This is a good TV\",\n",
    "        \"This is a good laptop\"]\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_doc = []\n",
    "for d in doc:\n",
    "    tokenized_doc.append(word_tokenize(d.lower()))\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ffee865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'love', 'data', 'science'], tags=[0]),\n",
       " TaggedDocument(words=['i', 'love', 'coding', 'in', 'python'], tags=[1]),\n",
       " TaggedDocument(words=['i', 'love', 'building', 'nlp', 'tool'], tags=[2]),\n",
       " TaggedDocument(words=['this', 'is', 'a', 'good', 'phone'], tags=[3]),\n",
       " TaggedDocument(words=['this', 'is', 'a', 'good', 'tv'], tags=[4]),\n",
       " TaggedDocument(words=['this', 'is', 'a', 'good', 'laptop'], tags=[5])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tokenized document into gensim formated tagged data\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd45cef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'love',\n",
       " 'good',\n",
       " 'a',\n",
       " 'is',\n",
       " 'this',\n",
       " 'in',\n",
       " 'data',\n",
       " 'science',\n",
       " 'coding',\n",
       " 'laptop',\n",
       " 'python',\n",
       " 'building',\n",
       " 'tv',\n",
       " 'tool',\n",
       " 'phone',\n",
       " 'nlp']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4, epochs = 100)\n",
    "# Save trained doc2vec model\n",
    "model.save(\"test_doc2vec.model\")\n",
    "## Load saved doc2vec model\n",
    "model= Doc2Vec.load(\"test_doc2vec.model\")\n",
    "## Print model vocabulary\n",
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bea1d001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.5019209980964661),\n",
       " (4, 0.47509756684303284),\n",
       " (1, 0.42233988642692566),\n",
       " (3, 0.2952742278575897),\n",
       " (0, 0.18391263484954834)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar doc \n",
    "test_doc = word_tokenize(\"That is a good computer\".lower())\n",
    "model.dv.most_similar(positive=[model.infer_vector(test_doc)],topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a47e5",
   "metadata": {},
   "source": [
    "## Data Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fdaf7d",
   "metadata": {},
   "source": [
    "Natural language processing requires you to first manually identify important sections of text or tag the text with specific labels to generate your training dataset. For example, you may want to identify the sentiment or intent of a text blurb, identify parts of speech, classify proper nouns like places and people, and identify text in images, PDFs, or other files. To do this, you can draw bounding boxes around text and then manually transcribe the text in your training dataset. Natural language processing models are used for sentiment analysis, entity name recognition, and optical character recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee92d06",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bedc9f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77dffd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>send_replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h91cqc4</td>\n",
       "      <td>Street-Spot5011</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1629105358</td>\n",
       "      <td>Personally, I’d talk to him abt it in a CIVIL ...</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>/r/AskReddit/comments/p4u4i2/if_you_saw_someon...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h91cufe</td>\n",
       "      <td>Puechamp</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1629105416</td>\n",
       "      <td>Slap his face real hard. Never burn a book in ...</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>/r/AskReddit/comments/p4u4i2/if_you_saw_someon...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h91cvff</td>\n",
       "      <td>4AcidRayne</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1629105429</td>\n",
       "      <td>Add that person to my mental \"let\\'s keep an e...</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>/r/AskReddit/comments/p4u4i2/if_you_saw_someon...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h91cwch</td>\n",
       "      <td>hate_most_of_you</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1629105442</td>\n",
       "      <td>\"do notn'tn't\"</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>t1_h91caod</td>\n",
       "      <td>/r/AskReddit/comments/p4u4i2/if_you_saw_someon...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h91cxbj</td>\n",
       "      <td>JacksonBoyd12</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1629105455</td>\n",
       "      <td>Cheer them on and give them another Bible to burn</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>t3_p4u4i2</td>\n",
       "      <td>/r/AskReddit/comments/p4u4i2/if_you_saw_someon...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id            author  subreddit retrieved_on  \\\n",
       "0    h91cqc4   Street-Spot5011  AskReddit   1629105358   \n",
       "1    h91cufe          Puechamp  AskReddit   1629105416   \n",
       "2    h91cvff        4AcidRayne  AskReddit   1629105429   \n",
       "3    h91cwch  hate_most_of_you  AskReddit   1629105442   \n",
       "4    h91cxbj     JacksonBoyd12  AskReddit   1629105455   \n",
       "\n",
       "                                        comment_text    link_id   parent_id  \\\n",
       "0  Personally, I’d talk to him abt it in a CIVIL ...  t3_p4u4i2   t3_p4u4i2   \n",
       "1  Slap his face real hard. Never burn a book in ...  t3_p4u4i2   t3_p4u4i2   \n",
       "2  Add that person to my mental \"let\\'s keep an e...  t3_p4u4i2   t3_p4u4i2   \n",
       "3                                     \"do notn'tn't\"  t3_p4u4i2  t1_h91caod   \n",
       "4  Cheer them on and give them another Bible to burn  t3_p4u4i2   t3_p4u4i2   \n",
       "\n",
       "                                           permalink  send_replies  \n",
       "0  /r/AskReddit/comments/p4u4i2/if_you_saw_someon...          True  \n",
       "1  /r/AskReddit/comments/p4u4i2/if_you_saw_someon...          True  \n",
       "2  /r/AskReddit/comments/p4u4i2/if_you_saw_someon...          True  \n",
       "3  /r/AskReddit/comments/p4u4i2/if_you_saw_someon...          True  \n",
       "4  /r/AskReddit/comments/p4u4i2/if_you_saw_someon...          True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df['comment_id'] = comments_df['comment_id'].str.strip('(\\'\\,)').astype(str)\n",
    "comments_df['author'] = comments_df['author'].str.strip('(\\'\\,)').astype(str)\n",
    "comments_df['subreddit'] = comments_df['subreddit'].str.strip('(\\'\\,)').astype(str)\n",
    "comments_df['retrieved_on'] = comments_df['retrieved_on'].str.strip('(\\'\\,)').astype(str)\n",
    "comments_df['comment_text'] = comments_df['comment_text'].str.strip('(\\'\\,)').astype(str)\n",
    "comments_df['link_id'] = comments_df['link_id'].str.strip('(\\'\\,)').astype(str)\n",
    "comments_df['parent_id'] = comments_df['parent_id'].str.strip('(\\'\\,)').astype(str)\n",
    "comments_df['permalink'] = comments_df['permalink'].str.strip('(\\'\\,)').astype(str)\n",
    "\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"A\")[[\"B\"]].agg(lambda x: x.astype(float).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254f282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882387ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'TE.txt':'TE', 'cs.txt':'EX'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "889af236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# random\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9600d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1816/918065424.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors_lockf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         super(Doc2Vec, self).__init__(\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9463b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
