{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfbeef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6dea3",
   "metadata": {},
   "source": [
    "### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ad6296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authentication(client_id, secret_key):\n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, secret_key)\n",
    "    return auth\n",
    "\n",
    "def authRequest(auth):\n",
    "    with open('pwd.txt', 'r') as f:\n",
    "        pwd = f.read()\n",
    "    \n",
    "    data = {\n",
    "    'grant_type' : 'password',\n",
    "    'username' : 'Resuri_988',\n",
    "    'password' : pwd\n",
    "    }\n",
    "    \n",
    "    headers = {'User-Agent' : 'MyAPI/0.0.1'}\n",
    "    \n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "    token = res.json()['access_token']\n",
    "    headers['Authorization'] = f'bearer {token}'\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f144040",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'R3jh6n-3nvEW-LiYIAtZ4w'\n",
    "secret_key = 'cUAbHuCsYaZUiMhWLIgzJRA1q-eJaw'\n",
    "\n",
    "# API request in JSON form\n",
    "auth = authentication(client_id, secret_key)\n",
    "headers = authRequest(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0f1e3",
   "metadata": {},
   "source": [
    "### Submissions\n",
    "Collect the most popular 100 subreddits and get 200 submissions from <b>past years</b> for each subreddit as the latest submissions doesn't have sufficient comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1db10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_subreddits_response(res):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for post in res.json()['data']['children']:\n",
    "        df = df.append({\n",
    "            'name': post['data']['name']\n",
    "        }, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def df_from_listings_response(res):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for post in res.json()['data']['children']:\n",
    "        df = df.append({\n",
    "            'name': post['data']['name']\n",
    "        }, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_controversial_past_year_submissions(subreddit, n):\n",
    "    response_list = []\n",
    "    params = {'limit': '10000', 't': 'year'}\n",
    "    epochs = n//100\n",
    "    \n",
    "    if n < 100:\n",
    "        epochs = 1\n",
    "    for _ in range(epochs):\n",
    "        response = requests.get(\"https://oauth.reddit.com/\" + subreddit + \"/controversial\", headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            error_text = \"Request returned an error: {} {}\".format(response.status_code, response.text)\n",
    "            print(response.status_code)\n",
    "            raise Exception(\"Unsuccessful Trial\")\n",
    "            \n",
    "        new_df = df_from_listings_response(response)\n",
    "        row = new_df.iloc[0]\n",
    "        params['after'] = row['name']\n",
    "    \n",
    "        append_to_csv(response.json(), 'submissions.csv')\n",
    "                    \n",
    "    return response_list\n",
    "\n",
    "def get_popular_subreddits(n):\n",
    "    response_list = []\n",
    "    params = {'limit': '10000'}\n",
    "    epochs = n//100\n",
    "    \n",
    "    if n < 100:\n",
    "        epochs = 1\n",
    "    for _ in range(epochs):\n",
    "        response = requests.get(\"https://oauth.reddit.com/subreddits/popular\", headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            error_text = \"Request returned an error: {} {}\".format(response.status_code, response.text)\n",
    "            print(response.status_code)\n",
    "            raise Exception(\"Unsuccessful Trial\")\n",
    "\n",
    "        new_df = df_from_subreddits_response(response)\n",
    "        row = new_df.iloc[0]\n",
    "        params['after'] = row['name']\n",
    "        \n",
    "        for item in response.json()['data']['children']:\n",
    "            for post in item['data']:\n",
    "                if post == \"display_name_prefixed\":\n",
    "                    response_list.append(item['data'].get(post))\n",
    "                        \n",
    "    return response_list\n",
    "\n",
    "def write_column_names(fileName):\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "    csvWriter.writerow(['id', 'kind', 'category', 'created_utc', 'author', 'name', 'subreddit_id', \n",
    "                        'subreddit_subscriber','subreddit', 'title', 'selftext', 'upvote_ratio','url','num_comments',\n",
    "                        'ups', 'downs', 'total_awards_received', 'score', 'created', 'num_crossposts'])\n",
    "    csvFile.close()\n",
    "    \n",
    "def append_to_csv(json_res, fileName):\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    print(\"Appending to csv ... \")\n",
    "\n",
    "    sr_post_list = json_res['data']['children']\n",
    "\n",
    "    for post in sr_post_list:\n",
    "        current_sr_post = post['data']\n",
    "\n",
    "        id_ = current_sr_post['id']\n",
    "        kind = post['kind']\n",
    "        category = current_sr_post['category']\n",
    "        created_utc = current_sr_post['created_utc']\n",
    "        author = current_sr_post['author']\n",
    "        name = current_sr_post['name']\n",
    "        subreddit_id = current_sr_post['subreddit_id']\n",
    "        subreddit_subscribers = current_sr_post['subreddit_subscribers']\n",
    "        subreddit = current_sr_post['subreddit']\n",
    "        title = current_sr_post['title']\n",
    "        selftext = current_sr_post['selftext']\n",
    "        upvote_ratio = current_sr_post['upvote_ratio']\n",
    "        url = current_sr_post['url']\n",
    "        num_comments = current_sr_post['num_comments']\n",
    "        ups = current_sr_post['ups']\n",
    "        downs = current_sr_post['downs']\n",
    "        total_awards_received = current_sr_post['total_awards_received']\n",
    "        score = current_sr_post['score']\n",
    "        created = current_sr_post['created']\n",
    "        num_crossposts = current_sr_post['num_crossposts']\n",
    "\n",
    "\n",
    "        # Assemble all data in a list\n",
    "        result = [id_, kind, category, created_utc, author, name, subreddit_id, subreddit_subscribers, subreddit, title, selftext, upvote_ratio, url, num_comments, ups, downs, total_awards_received, score, created, num_crossposts]\n",
    "\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(result)\n",
    "\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_column_names('submissions.csv')\n",
    "subreddits = get_popular_subreddits(100)\n",
    "submissions = [get_controversial_past_year_submissions(sr, 200) for sr in subreddits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6fc98",
   "metadata": {},
   "source": [
    "### Comments\n",
    "Retrieving comments is relatively difficult using Reddit API as it limit the data request for maximum of 10000, thus using third party API such as Pushshift will ease the comments extraction and produce a larger volume of comments dataset for particular post/submission. \n",
    "\n",
    "In this section, we will retrieve comments for each particular post from each submission. These comments will eventually be stored in the databases along with it's essential information as JSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "818e4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def get_all_post_ids(filename):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        post_ids = df.id.tolist()\n",
    "        return post_ids\n",
    "    except:\n",
    "        print(\"An error occur\")\n",
    "        \n",
    "def get_comments_with_post_ids(post_ids):\n",
    "    # 1.get comment ids based on post ids\n",
    "    # 2.get comment text based on comment ids\n",
    "    CHUNK_SIZE = 10000\n",
    "    comment_ids = [api.search_submission_comment_ids(ids=ids_chunk) for ids_chunk in list(chunks(post_ids, CHUNK_SIZE))]\n",
    "    print(\"Successful Attempt on generating all comment ids ... \")\n",
    "    comment_body_list = [api.search_comments(ids=body_chunk) for body_chunk in list(chunks(comment_ids, CHUNK_SIZE))]\n",
    "    print(\"Successful Attempt on generating all comments ... \")\n",
    "    return comment_body_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 96.00% - Requests: 100 - Batches: 10 - Items Remaining: 9904\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 91.50% - Requests: 200 - Batches: 20 - Items Remaining: 9817\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 92.33% - Requests: 300 - Batches: 30 - Items Remaining: 9723\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 91.50% - Requests: 400 - Batches: 40 - Items Remaining: 9634\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 91.00% - Requests: 500 - Batches: 50 - Items Remaining: 9545\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 92.00% - Requests: 600 - Batches: 60 - Items Remaining: 9448\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 92.00% - Requests: 700 - Batches: 70 - Items Remaining: 9356\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 91.88% - Requests: 800 - Batches: 80 - Items Remaining: 9265\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 91.78% - Requests: 900 - Batches: 90 - Items Remaining: 9174\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 91.80% - Requests: 1000 - Batches: 100 - Items Remaining: 9082\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 91.82% - Requests: 1100 - Batches: 110 - Items Remaining: 8990\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 91.50% - Requests: 1200 - Batches: 120 - Items Remaining: 8902\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 90.92% - Requests: 1300 - Batches: 130 - Items Remaining: 8818\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.93% - Requests: 1400 - Batches: 140 - Items Remaining: 8741\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.53% - Requests: 1500 - Batches: 150 - Items Remaining: 8657\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.62% - Requests: 1600 - Batches: 160 - Items Remaining: 8566\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.53% - Requests: 1700 - Batches: 170 - Items Remaining: 8478\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.83% - Requests: 1800 - Batches: 180 - Items Remaining: 8383\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.74% - Requests: 1900 - Batches: 190 - Items Remaining: 8295\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.90% - Requests: 2000 - Batches: 200 - Items Remaining: 8202\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 90.14% - Requests: 2100 - Batches: 210 - Items Remaining: 8107\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.91% - Requests: 2200 - Batches: 220 - Items Remaining: 8022\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 90.04% - Requests: 2300 - Batches: 230 - Items Remaining: 7929\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.92% - Requests: 2400 - Batches: 240 - Items Remaining: 7842\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 90.08% - Requests: 2500 - Batches: 250 - Items Remaining: 7748\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.85% - Requests: 2600 - Batches: 260 - Items Remaining: 7664\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.63% - Requests: 2700 - Batches: 270 - Items Remaining: 7580\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.89% - Requests: 2800 - Batches: 280 - Items Remaining: 7483\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 89.83% - Requests: 2900 - Batches: 290 - Items Remaining: 7395\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 88.77% - Requests: 3000 - Batches: 300 - Items Remaining: 7337\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 88.61% - Requests: 3100 - Batches: 310 - Items Remaining: 7253\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 88.50% - Requests: 3200 - Batches: 320 - Items Remaining: 7168\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 88.33% - Requests: 3300 - Batches: 330 - Items Remaining: 7085\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 88.21% - Requests: 3400 - Batches: 340 - Items Remaining: 7001\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 87.97% - Requests: 3500 - Batches: 350 - Items Remaining: 6921\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 87.67% - Requests: 3600 - Batches: 360 - Items Remaining: 6844\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 87.32% - Requests: 3700 - Batches: 370 - Items Remaining: 6769\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.47% - Requests: 3800 - Batches: 380 - Items Remaining: 6714\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.56% - Requests: 3900 - Batches: 390 - Items Remaining: 6624\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.60% - Requests: 4000 - Batches: 400 - Items Remaining: 6536\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.66% - Requests: 4100 - Batches: 410 - Items Remaining: 6447\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.69% - Requests: 4200 - Batches: 420 - Items Remaining: 6359\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.88% - Requests: 4300 - Batches: 430 - Items Remaining: 6264\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 87.00% - Requests: 4400 - Batches: 440 - Items Remaining: 6172\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.91% - Requests: 4500 - Batches: 450 - Items Remaining: 6089\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.59% - Requests: 4600 - Batches: 460 - Items Remaining: 6017\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.32% - Requests: 4700 - Batches: 470 - Items Remaining: 5943\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.44% - Requests: 4800 - Batches: 480 - Items Remaining: 5851\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.39% - Requests: 4900 - Batches: 490 - Items Remaining: 5767\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.40% - Requests: 5000 - Batches: 500 - Items Remaining: 5680\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.47% - Requests: 5100 - Batches: 510 - Items Remaining: 5590\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.58% - Requests: 5200 - Batches: 520 - Items Remaining: 5498\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.57% - Requests: 5300 - Batches: 530 - Items Remaining: 5412\n",
      "INFO:pmaw.PushshiftAPIBase:Checkpoint:: Success Rate: 86.50% - Requests: 5400 - Batches: 540 - Items Remaining: 5329\n"
     ]
    }
   ],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()\n",
    "write_column_names('comments.csv')\n",
    "post_ids = get_all_post_ids('submissions.csv')\n",
    "comments = get_comments_with_post_ids(post_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f055053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3331371",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-sql\n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ffaf4",
   "metadata": {},
   "source": [
    "# Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.read_csv('submissions.csv')\n",
    "comments = pd.read_csv('comments.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25217c",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f13bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlcol(dfparam):    \n",
    "    dtypedict = {}\n",
    "    for i,j in zip(dfparam.columns, dfparam.dtypes):\n",
    "        if \"object\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.NVARCHAR(length=255)})\n",
    "                                 \n",
    "        if \"datetime\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.DateTime()})\n",
    "\n",
    "        if \"float\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.Float(precision=3, asdecimal=True)})\n",
    "\n",
    "        if \"int\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.INT()})\n",
    "    return dtypedict\n",
    "\n",
    "outputdict = sqlcol(df)    \n",
    "column_errors.to_sql('load_errors', \n",
    "                     push_conn, \n",
    "                     if_exists = 'append', \n",
    "                     index = False, \n",
    "                     dtype = outputdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2030b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee4856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86621ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pwd.txt', 'r') as f:\n",
    "    pwd = f.read()\n",
    "        \n",
    "%sql dialect+driver://username:pwd@host:port/reddit\n",
    "# Example format\n",
    "%sql postgresql://postgres:password123@localhost/dvdrental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82807100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "engine = create_engine('dialect+driver://username:password@host:port/database')\n",
    "# Example format\n",
    "engine = create_engine('postgresql://postgres:password123@localhost/dvdrental')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d4864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a536aae4",
   "metadata": {},
   "source": [
    "### Insert data into PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a0577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE submissions (\n",
    "    id BIGSERIAL NOT NULL PRIMARY KEY,\n",
    "    post_id VARCHAR(100) NOT NULL,\n",
    "    kind VARCHAR(100) NOT NULL,\n",
    "    category VARCHAR(100),\n",
    "    created_utc INT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7ea68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e3849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
